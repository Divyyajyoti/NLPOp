# Prototype_22041
# Humanised the output generated by the program for the end user, optimisation type identification
# New session for new problems, (prompting user for missing values)'
# Stochastic programming

import streamlit as st
import google.generativeai as genai
from scipy.optimize import linprog, minimize, linear_sum_assignment
from itertools import permutations
import os
import re
import numpy as np
import pandas as pd

# --- LangChain Agent Integration ---
from langchain.llms import HuggingFaceHub
from langchain.agents import initialize_agent, Tool
from langchain.agents.agent_types import AgentType
from langchain.tools import DuckDuckGoSearchRun
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

st.set_page_config(page_title="Optimization & Chat Assistant", layout="wide")
st.title("ü§ñ Optimization & Chat Assistant")

# Set your Hugging Face API token (free tier)
os.environ["HUGGINGFACEHUB_API_TOKEN"] = os.getenv("HUGGINGFACEHUB_API_TOKEN", "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")

# Initialize the free LLM (FLAN-T5-XL from Hugging Face)
agent_llm = HuggingFaceHub(repo_id="google/flan-t5-xl", model_kwargs={"temperature": 0.5, "max_length": 1024})

# Define tools the agent can use
search = DuckDuckGoSearchRun()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="Useful for answering general knowledge or current event questions"
    ),
    Tool(
        name="Optimization Solver",
        func=lambda x: "Please switch to Optimization mode to solve optimization problems",
        description="Useful for solving optimization problems"
    )
]

# Initialize the LangChain agent
langchain_agent = initialize_agent(
    tools=tools,
    llm=agent_llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)


# Function to query the LangChain agent with fallback to Gemini
def ask_general_agent(query):
    try:
        # First try with LangChain agent
        try:
            response = langchain_agent.run(query)
            if response and "I don't know" not in response and "not sure" not in response.lower():
                return response
        except Exception as e:
            print(f"LangChain agent error: {e}")

        # Fallback to Gemini
        model = genai.GenerativeModel('models/learnlm-1.5-pro-experimental')
        response = model.generate_content(query)
        return response.text
    except Exception as e:
        return f"‚ùå Error generating response: {e}"


# --- Configure Google GenAI ---
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    st.error("Google API key not found. Set it as an environment variable.")
    st.stop()

genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel('models/learnlm-1.5-pro-experimental')


# Mode selection
mode = st.sidebar.radio("Select Mode:", ["Optimization", "General Chat"])

if mode == "General Chat":
    st.header("üí¨ General Chat")
    st.markdown("Ask me anything and I'll do my best to answer!")

    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []

    user_input = st.chat_input("Ask me anything...")

    if user_input:
        st.session_state.chat_history.append(("user", user_input))
        with st.spinner("Thinking..."):
            response = ask_general_agent(user_input)
            st.session_state.chat_history.append(("assistant", response))

    # Display chat history
    for role, message in st.session_state.chat_history:
        with st.chat_message(role):
            st.write(message)

    st.stop()  # Stop execution here for chat mode

# --- Rest of the optimization code remains the same ---
# (All your existing optimization code goes here, starting with OPT_TYPES definition)

# --- Optimization Type Detection ---
OPT_TYPES = {
    "linear_programming": False,
    "integer_programming": False,
    "nonlinear_programming": False,
    "quadratic_programming": False,
    "convex_programming": False,
    "combinatorial_optimization": False,
    "dynamic_programming": False,
    "stochastic_optimization": False,
    "multi_objective_optimization": False,
    "set_covering": False,
    "stochastic_programming": False
}

count = []


def detect_optimization_type(problem_statement):
    prompt = f"""
    You are an expert in optimization theory. Read the following problem statement and determine which types of optimization problems it involves.

    ---
    {problem_statement}
    ---

    In addition to standard categories (linear programming, integer programming, combinatorial optimization, etc.), **pay special attention to problems resembling Set Covering Problems**, even if they are not explicitly named.

    Set covering problems typically involve:
    - A universe of required elements (e.g., skills, cities, resources).
    - A collection of sets (e.g., candidates, facilities, resources) covering parts of the universe.
    - The objective is to select the minimal number of sets to cover the entire universe.

    If you detect this structure, **toggle 'set_covering' to True** even if the problem does not use the words "set covering."

    Also, if the problem involves uncertainty (for example, random demand, uncertain costs, probabilistic events), detect it as a **stochastic_programming** problem. 

    Typical signs:
    - Demand, costs, or other parameters have **different scenarios**.
    - Decisions have to be made **before** knowing the actual scenario.
    - The objective involves **expected values** or **chance constraints**.

    If such uncertainty is present, set `"stochastic_programming": True.


    Return a Python dictionary in the following format, setting only the relevant types to True:

    {OPT_TYPES}

    Respond ONLY with the dictionary. Do not include explanation.
    """

    model = genai.GenerativeModel('models/learnlm-1.5-pro-experimental')
    try:
        response = model.generate_content(prompt)
        match = re.search(r'\{.*\}', response.text, re.DOTALL)
        if match:
            detected_types = eval(match.group(0))
            # Basic heuristic: if the problem mentions non-linear terms. Improve this for better accuracy.
            if re.search(r'[a-zA-Z]\^\d+|[a-zA-Z]+\*[a-zA-Z]+|sin\(|cos\(|exp\(|log\(', problem_statement):
                detected_types["nonlinear_programming"] = True
            if "set cover" in problem_statement.lower() or "set covering" in problem_statement.lower():
                detected_types["set_covering"] = True
            return detected_types
    except Exception as e:
        print(f"Error in detect_optimization_type: {e}")  # Add error logging
        pass
    return OPT_TYPES  # fallback


import ast


def simplify_math_expressions(text):
    pattern = re.compile(r'\b(\d+(?:\.\d+)?)\s*\*\s*(\d+(?:\.\d+)?)\b')
    return pattern.sub(lambda m: str(float(m.group(1)) * float(m.group(2))), text)


def extract_lpp_from_text(text):
    model = genai.GenerativeModel('models/learnlm-1.5-pro-experimental')

    prompt = f"""
    You are a world-class mathematical assistant designed to extract structured Linear Programming Problems (LPPs) from natural language.

    ---

    Your task involves **three stages**:

    ### üîÅ Stage 1: Unit Standardization
    1. Convert all quantities into **SI units**.
    2. **Reframe the problem text using SI units** for internal processing and validation.
    3. **Before final output**, convert all variables and constraints **back into their original units** as used in the question to avoid confusing the user.

    ---

    ### üß† Stage 2: LPP Extraction
    From the question below, extract the following LPP components:
    - Objective function coefficients for **both** `maximize` and `minimize` objectives:
      - If only one is mentioned, infer the other by negating or mirroring as needed.
    - Inequality constraints: matrix `A_ub`, vector `b_ub`
    - Equality constraints: matrix `A_eq`, vector `b_eq`
    - Variable bounds: list of `(lower, upper)` tuples. Default: `(0, None)` if not mentioned.
    - Variable names: e.g., `["x1", "x2", ...]` (generate meaningful names when possible).
    - Constraint names: e.g., `["Raw Material Constraint", "Budget Constraint", ...]`
    - Objective type: `"maximize"`, `"minimize"`, or `"mixed"`
    Ensure all values (coefficients, RHS, bounds) are floats.

    ---

    ### üîÅ Stage 3: Matrix Verification (5-Pass Loop)
    Double-check the integrity of all matrices:
    - **Verify variable-to-column alignment**
    - **Ensure shape consistency** between `A_ub`, `b_ub`, `A_eq`, `b_eq`
    - **Confirm that all constraints, objective coefficients, and bounds reflect the original logic**
    - Repeat this validation logic **five times** before finalizing the dictionary to ensure consistency and correctness.

    ---

    ### üìù Input:
    \"\"\"{text}\"\"\"

    ---

    ### ‚úÖ Final Output Format
    Output ONLY a **valid Python dictionary** (no explanation, no markdown, no comments), strictly following this schema:

    {{
        "c_max": [float, ...],
        "c_min": [float, ...],
        "A_ub": [[float, ...], ...],
        "b_ub": [float, ...],
        "A_eq": [[float, ...], ...] or None,
        "b_eq": [float, ...] or None,
        "bounds": [(float, float or None), ...],
        "objective": "maximize" or "minimize" or "mixed",
        "variable_names": ["x1", "x2", ...],
        "constraint_names": ["Constraint 1", "Constraint 2", ...]
    }}

    Return only the dictionary. Do not include code blocks, comments, or any other content.
    ---

    ### üö® Missing Values Handling
    If any value (such as `b_eq`, `A_eq`, or variable bounds) is inferred as `None`, include a placeholder and **clearly mark the corresponding variable or constraint name**.

    The downstream logic will prompt the user to optionally fill in the missing values. So do not guess ‚Äî just flag them with `None`.

    Only return the final LPP dictionary as described earlier.
    """

    response = model.generate_content(prompt)
    count.append(model.count_tokens(response.text).total_tokens)

    try:
        match = re.search(r'\{.*\}', response.text, re.DOTALL)

        if match:
            raw_dict = match.group(0)
            raw_dict = simplify_math_expressions(raw_dict)

            try:
                parsed_dict = ast.literal_eval(raw_dict)
            except Exception:
                parsed_dict = eval(raw_dict)

            st.session_state.history.append(("assistant", parsed_dict))
            return parsed_dict, None
        else:
            return None, "Invalid LPP format returned (no dictionary match)."

    except Exception as e:
        return None, f"‚ùå Error parsing LPP: {e}"


def solve_lpp(lpp_dict, alpha=0.5):
    c_max = lpp_dict.get('c_max')
    c_min = lpp_dict.get('c_min')
    A_ub = lpp_dict.get('A_ub')
    b_ub = lpp_dict.get('b_ub')
    A_eq = lpp_dict.get('A_eq')
    b_eq = lpp_dict.get('b_eq')
    bounds = lpp_dict.get('bounds')
    objective = lpp_dict.get('objective')

    num_vars = len(c_max) if c_max else len(c_min)
    if not bounds or len(bounds) != num_vars:
        bounds = [None for _ in range(num_vars)]

    if objective == 'maximize':
        c = [-val for val in c_max]
    elif objective == 'minimize':
        c = c_min
    elif objective == 'mixed':
        c = [(alpha * -x) + ((1 - alpha) * y) for x, y in zip(c_max, c_min)]
    else:
        return "Unknown objective type.", None, None

    try:
        res = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success:
            if objective == 'maximize':
                res.fun = -res.fun
            return None, res.fun, res.x
        else:
            return f"LPP solving failed: {res.message}", None, None
    except ValueError as e:
        return f"Error solving LPP: {e}", None, None


def display_constraints(lpp_dict):
    details = []
    var_names = lpp_dict.get('variable_names') or [f"x{i + 1}" for i in
                                                   range(len(lpp_dict.get('c_max', lpp_dict.get('c_min', []))))]
    con_names = lpp_dict.get('constraint_names', [])

    if lpp_dict.get('A_ub'):
        for idx, (row, b) in enumerate(zip(lpp_dict['A_ub'], lpp_dict['b_ub'])):
            constraint = " + ".join(f"{round(coef, 2)}{var_names[i]}" for i, coef in enumerate(row))
            name = con_names[idx] if idx < len(con_names) else f"Constraint {idx + 1}"
            details.append(f"{name}: {constraint} <= {b}")
    if lpp_dict.get('A_eq'):
        start_idx = len(lpp_dict.get('A_ub') or [])
        for i, (row, b) in enumerate(zip(lpp_dict['A_eq'], lpp_dict['b_eq'])):
            constraint = " + ".join(f"{round(coef, 2)}{var_names[j]}" for j, coef in enumerate(row))
            idx = start_idx + i
            name = con_names[idx] if idx < len(con_names) else f"Constraint {idx + 1}"
            details.append(f"{name}: {constraint} = {b}")
    return "\n".join(details) or "No constraints found."


def format_solution(opt_val, opt_vars, objective, lpp_dict):
    if opt_val is None or opt_vars is None:
        return "No feasible solution found."

    var_names = lpp_dict.get('variable_names') or [f"x{i + 1}" for i in range(len(opt_vars))]
    var_details = "\n".join([f"  - {name}: {val:.2f}" for name, val in zip(var_names, opt_vars)])

    summary = ""
    if objective == 'maximize' and lpp_dict.get('c_max'):
        terms = [f"{round(coef, 2)}x{i + 1}" for i, coef in enumerate(lpp_dict['c_max'])]
        summary += "**Objective Function:** Maximize Z = " + " + ".join(terms) + "\n"
    elif objective == 'minimize' and lpp_dict.get('c_min'):
        terms = [f"{round(coef, 2)}x{i + 1}" for i, coef in enumerate(lpp_dict['c_min'])]
        summary += "**Objective Function:** Minimize Z = " + " + ".join(terms) + "\n"
    elif objective == 'mixed':
        c_max = lpp_dict.get('c_max')
        c_min = lpp_dict.get('c_min')
        if c_max and c_min:
            terms = [f"(Œ±*-{x} + (1-Œ±)*{y})x{i + 1}" for i, (x, y) in enumerate(zip(c_max, c_min))]
            summary += "**Objective Function:** Mixed = " + " + ".join(terms) + "\n"

    summary += "\n**Constraints:**\n" + display_constraints(lpp_dict) + "\n\n"
    result_text = f"Optimal Value: **{opt_val:.2f}**\n\nVariable Values:\n{var_details}"

    return summary + result_text


def modify_lpp(session_problem, user_input):
    model = genai.GenerativeModel('models/learnlm-1.5-pro-experimental')
    prompt = f"""
    You are assisting in modifying a Linear Programming Problem (LPP). Here is the existing LPP in dictionary format:

    {session_problem}

    Based on this user instruction:
    "{user_input}"

    Return an updated version of the dictionary **with only the necessary changes made**. 
    DO NOT remove or omit any fields from the original unless asked explicitly. Maintain structure integrity.

    Return ONLY the Python dictionary with changes implemented. No explanation or extra text.
    """
    response = model.generate_content(prompt)
    count.append(model.count_tokens(response.text).total_tokens)
    try:
        match = re.search(r'\{.*\}', response.text, re.DOTALL)
        if match:
            return eval(match.group(0)), None
        else:
            return None, "Failed to parse modified LPP."
    except Exception as e:
        return None, f"Error parsing modified LPP: {e}"


# LLM prompt generator (mocking GPT for now ‚Äî integrate your own model API here)
def generate_prompt(missing_type, variable_name=None, constraint_name=None):
    if missing_type == "A_eq":
        return model.generate_content(
            f"What is the coefficient of variable '{variable_name}' in constraint '{constraint_name}'?").text
    elif missing_type == "b_eq":
        return model.generate_content(f"What is the right-hand side value for constraint '{constraint_name}'?").text
    else:
        return model.generate_content(f"Please enter a value.").text



# --- Non-Linear Programming Functions ---
def extract_nlp_components(problem_statement):
    prompt = f"""
    You are an expert in extracting components of a Non-Linear Programming (NLP) problem from natural language.

    ---
    From the following problem statement, identify and extract:

    - **Objective Function (as a Python string representing a lambda function with variables 'x[0]', 'x[1]', ...):** Express the function to be minimized or maximized.
    - **Objective Type:** "minimize" or "maximize".
    - **Constraints (a list of dictionaries, each with 'type' ('eq' or 'ineq') and 'fun' (as a Python string representing a lambda function with 'x')):** Represent the equality and inequality constraints.
    - **Initial Guess (a list of initial values for the variables):** Provide a reasonable starting point for the optimization.
    - **Variable Names (a list of names for the variables):**
    - **Constraint Descriptions (a list of human-readable descriptions for the constraints):**

    If any component cannot be reliably extracted, indicate with None.

    For the given problem:
    "Nestl√© produces two types of chocolates‚ÄîMilk Chocolate and Dark Chocolate‚Äîand wants to maximize its profit while considering non-linear production costs. The selling price per unit is ‚Çπ200 for Milk Chocolate and ‚Çπ250 for Dark Chocolate. However, the cost per unit follows a non-linear pattern: ‚Çπ(50 + 0.01M¬≤) for Milk Chocolate and ‚Çπ(70 + 0.008D¬≤) for Dark Chocolate. The company has 10,000 kg of cocoa, with each Milk Chocolate requiring 3 kg and each Dark Chocolate needing 5 kg. The goal is to determine the optimal number of chocolates to produce while ensuring total cocoa usage does not exceed the limit."

    A potential output structure would be (adjust variable names and function definitions based on your interpretation):

    {{
        "objective_function": "lambda x: (200 * x[0]) + (250 * x[1]) - (50 + 0.01 * x[0]**2) - (70 + 0.008 * x[1]**2)",
        "objective_type": "maximize",
        "constraints": [
            {{'type': 'ineq', 'fun': "lambda x: 10000 - (3 * x[0] + 5 * x[1])" }}
        ],
        "initial_guess": [100, 100],
        "variable_names": ["Milk Chocolate", "Dark Chocolate"],
        "constraint_descriptions": ["Total cocoa usage within limit"]
    }}

    ---
    Problem Statement:
    \"\"\"{problem_statement}\"\"\"

    ---
    Return ONLY a Python dictionary in the specified format. Ensure that 'objective_function' and the 'fun' part of 'constraints' are strings representing lambda functions. Do not include any explanation or extra text.
    """
    model = genai.GenerativeModel('models/learnlm-1.5-pro-experimental')
    try:
        response = model.generate_content(prompt)
        count.append(model.count_tokens(response.text).total_tokens)
        match = re.search(r'\{.*\}', response.text, re.DOTALL)
        if match:
            return eval(match.group(0)), None
        else:
            return None, "Invalid NLP format returned."
    except Exception as e:
        return None, f"Error parsing NLP: {e}"


def solve_nlp(nlp_dict):
    objective_func_str = nlp_dict.get('objective_function')
    objective_type = nlp_dict.get('objective_type', 'minimize')
    constraints_data = nlp_dict.get('constraints', [])
    initial_guess = nlp_dict.get('initial_guess')

    if not objective_func_str or not initial_guess:
        return "Objective function or initial guess not provided.", None, None

    try:
        objective_function = eval(objective_func_str)
    except Exception as e:
        return f"Error evaluating objective function: {e}", None, None

    formatted_constraints = []
    if constraints_data:
        for con in constraints_data:
            try:
                func = eval(con['fun'])
                formatted_constraints.append({'type': con['type'], 'fun': func})
            except Exception as e:
                return f"Error evaluating constraint: {e}", None, None

    if objective_type == 'maximize':
        def negative_objective(x):
            return -objective_function(x)

        solver_objective = negative_objective
    else:
        solver_objective = objective_function

    try:
        result = minimize(solver_objective, initial_guess, constraints=formatted_constraints, method='SLSQP')
        if result.success:
            optimal_value = result.fun if objective_type == 'minimize' else -result.fun
            return None, optimal_value, result.x
        else:
            return f"NLP solving failed: {result.message}", None, None
    except Exception as e:
        return f"Error solving NLP: {e}", None, None


def format_nlp_solution(opt_val, opt_vars, objective, nlp_dict):
    if opt_val is None or opt_vars is None:
        return "No feasible solution found."

    var_names = nlp_dict.get('variable_names') or [f"x{i + 1}" for i in range(len(opt_vars))]
    var_details = "\n".join([f"  - {name}: {val:.4f}" for name, val in zip(var_names, opt_vars)])
    objective_str = nlp_dict.get('objective_function_original',
                                 "Objective Function")  # Consider storing original string

    summary = f"**Objective Function:** {objective_str} ({objective})\n\n"
    if nlp_dict.get('constraint_descriptions'):
        summary += "**Constraints:**\n" + "\n".join(
            [f"- {desc}" for desc in nlp_dict['constraint_descriptions']]) + "\n\n"

    result_text = f"Optimal Value: **{opt_val:.4f}**\n\nVariable Values:\n{var_details}"
    return summary + result_text


def modify_nlp(session_nlp, user_input):
    model = genai.GenerativeModel('models/learnlm-1.5-pro-experimental')
    prompt = f"""
    You are assisting in modifying a Non-Linear Programming (NLP) problem. Here is the existing NLP in dictionary format:

    {session_nlp}

    Based on this user instruction:
    "{user_input}"

    Return an updated version of the dictionary **with only the necessary changes made**.
    Ensure that the 'objective_function' and constraint 'fun' values remain as Python-executable strings (lambda functions).
    DO NOT remove or omit any fields from the original unless asked explicitly. Maintain structure integrity.

    Return ONLY the Python dictionary with changes implemented. No explanation or extra text.
    """
    response = model.generate_content(prompt)
    count.append(model.count_tokens(response.text).total_tokens)
    try:
        match = re.search(r'\{.*\}', response.text, re.DOTALL)
        if match:
            return eval(match.group(0)), None
        else:
            return None, "Failed to parse modified NLP."
    except Exception as e:
        return None, f"Error parsing modified NLP: {e}"


def modify_scp(problem_type, user_input):
    prompt = f"""
    You are assisting in modifying a set covering optimisation problem. Here is the existing problem in dictionary format:

    {problem_type}

    Based on this user instruction:
    "{user_input}"

    Return an updated version of the dictionary **with only the necessary changes made**.
    Ensure that the 'objective_function' and constraint 'fun' values remain as Python-executable strings (lambda functions).
    DO NOT remove or omit any fields from the original unless asked explicitly. Maintain structure integrity.

    Return ONLY the Python dictionary with changes implemented. No explanation or extra text.
    """
    response = model.generate_content(prompt)
    try:
        match = re.search(r'\{.*\}', response.text, re.DOTALL)
        if match:
            return eval(match.group(0)), None
        else:
            return None, "Failed to parse modified set covering problem."
    except Exception as e:
        print(f"Error in modify_scp: {e}")  # Add error logging
        return None, f"Error parsing modified NLP: {e}"


# --- Combinatorial Optimization Functions ---

def solve_knapsack(values, weights, capacity, item_names=None):
    """
    Solves the 0/1 knapsack problem using dynamic programming.

    values: List of item values (integers)
    weights: List of item weights (integers)
    capacity: Maximum weight capacity (integer)
    item_names: Optional list of item labels/names
    """
    # Input validation
    if not isinstance(capacity, (int, float)) or capacity != int(capacity):
        raise ValueError("Capacity must be an integer.")
    if not all(float(v).is_integer() for v in values):
        raise ValueError("All values must be integers.")
    if not all(float(w).is_integer() for w in weights):
        raise ValueError("All weights must be integers.")

    values = [int(v) for v in values]
    weights = [int(w) for w in weights]
    capacity = int(capacity)
    n = len(values)

    if item_names is None:
        item_names = [f"Item {i + 1}" for i in range(n)]
    elif len(item_names) != n:
        raise ValueError("Length of item_names must match number of items.")

    # DP Table
    dp = np.zeros((n + 1, capacity + 1), dtype=int)

    for i in range(1, n + 1):
        for w in range(capacity + 1):
            if weights[i - 1] <= w:
                dp[i][w] = max(dp[i - 1][w],
                               dp[i - 1][w - weights[i - 1]] + values[i - 1])
            else:
                dp[i][w] = dp[i - 1][w]

    # Backtrack to find selected items
    optimal_value = int(dp[n][capacity])
    chosen_items = []
    w = capacity
    for i in range(n, 0, -1):
        if dp[i][w] != dp[i - 1][w]:
            chosen_items.append(item_names[i - 1])
            w -= weights[i - 1]

    return optimal_value, chosen_items[::-1]


def display_knapsack_solution(values, weights, item_names, chosen_items):
    """
    Displays a formatted table of the knapsack result in Streamlit.

    values: List of item values
    weights: List of item weights
    item_names: List of item labels/names
    chosen_items: List of selected item names (as returned by solve_knapsack)
    """
    data = []
    for name, val, wt in zip(item_names, values, weights):
        selected = "‚úÖ Yes" if name in chosen_items else "‚ùå No"
        data.append({"Item": name, "Value": val, "Weight": wt, "Selected": selected})

    df = pd.DataFrame(data)
    st.subheader("üìã Knapsack Decision Table")
    st.dataframe(df, use_container_width=True)  # or use st.table(df)


def solve_tsp(distance_matrix):
    """
    Solves the Traveling Salesman Problem (TSP) using a brute force approach.
    Represents no direct path with infinity (np.inf).
    distance_matrix: A 2D array where distance_matrix[i][j] is the distance between city i and city j.
                     Use np.inf to represent no direct path.
    """
    n = len(distance_matrix)

    # Check if the distance matrix is square
    if len(distance_matrix) != len(distance_matrix[0]):
        return "Error: Distance matrix must be square.", None

    # Check if the distance matrix is empty
    if n == 0:
        return "Error: Distance matrix cannot be empty.", None

    # Check if the distance matrix has only one city
    if n == 1:
        return 0, [0]

    if n > 10:
        return "Warning: TSP instance is relatively large for brute force. Solution might take a very long time.", None

    cities = list(range(n))
    min_distance = float('inf')
    best_path = None

    for perm in permutations(cities):
        current_distance = 0
        valid_path = True
        for i in range(n):
            current_city = perm[i]
            next_city = perm[(i + 1) % n]  # Cycle back to the start

            distance = distance_matrix[current_city][next_city]
            if np.isinf(distance):
                valid_path = False
                break
            current_distance += distance

        if valid_path and current_distance < min_distance:
            min_distance = current_distance
            best_path = perm

    if min_distance == float('inf'):
        return "Error: No valid TSP tour found (possibly due to disconnected cities or all paths being infinite).", None

    return min_distance, list(best_path)


def solve_set_covering(sets, universe):
    """
    Solves the Set Covering problem using a greedy approximation algorithm.

    Args:
        sets: A list of sets, where each set is a list of elements from the universe.
        universe: The set of all elements that need to be covered.

    Returns:
        A tuple containing:
            - A list of the indices of the sets chosen for the cover.
            - The cost of the cover (number of sets chosen).
    """
    uncovered = set(universe)
    cover = []
    set_indices = list(range(len(sets)))  # Keep track of original indices

    while uncovered:
        best_set_index = -1
        max_elements_covered = 0

        for i, s in enumerate(sets):
            elements_covered = len(set(s) & uncovered)
            if elements_covered > max_elements_covered:
                max_elements_covered = elements_covered
                best_set_index = i

        if best_set_index == -1:
            return "Error: No set covers the remaining elements.", None

        cover.append(best_set_index)
        uncovered -= set(sets[best_set_index])

    return cover, len(cover)


def solve_assignment_problem(cost_matrix):
    """
    Solves the Assignment Problem using the Hungarian algorithm (implemented in scipy).
    cost_matrix: A 2D array where cost_matrix[i][j] is the cost of assigning worker i to job j.
    """
    try:
        row_ind, col_ind = linear_sum_assignment(cost_matrix)
        optimal_cost = cost_matrix[row_ind, col_ind].sum()
        assignment = list(zip(row_ind, col_ind))  # List of (worker, job) pairs
        return optimal_cost, assignment
    except Exception as e:
        return f"Error solving assignment problem: {e}", None


def format_tsp_solution(distance, path):
    """Formats the TSP solution into a human-readable string."""
    path_str = " -> ".join(map(str, path))
    return f"Optimal TSP Path: {path_str}\nTotal Distance: {distance:.2f}"


def format_assignment_solution(cost, assignment):
    """Formats the Assignment Problem solution into a human-readable string."""
    assignment_str = "\n".join([f"Worker {w} assigned to Job {j}" for w, j in assignment])
    return f"Optimal Assignment:\n{assignment_str}\nTotal Cost: {cost:.2f}"


def format_set_covering_solution(cover_indices, cost, sets):
    """Formats the Set Covering Problem solution into a human-readable string."""
    if isinstance(cover_indices, str):
        return cover_indices  # Return the error message

    covered_sets = [f"Set {i + 1}: {sets[i]}" for i in cover_indices]  # +1 for 1-based indexing
    sets_used = ", ".join(map(str, [i + 1 for i in cover_indices]))
    return f"Sets chosen: {sets_used}\nCovered Sets:\n" + "\n".join(covered_sets) + f"\nTotal Sets (Cost): {cost}"


def extract_combinatorial_data(problem_statement):
    """
    Uses AI to extract the problem type and data for combinatorial optimization.
    """
    model = genai.GenerativeModel('models/learnlm-1.5-pro-experimental')
    prompt = f"""
    You are an expert in combinatorial optimization. Your task is to analyze the following problem statement and:

    1.  Identify the problem type: Determine if it's a Traveling Salesman Problem (TSP), Assignment Problem, Knapsack Problem, or set covering problem.
    2.  Extract the data: Extract the relevant data for the identified problem type. This might include a distance matrix (for TSP), a cost matrix (for Assignment), or values, weights, item names list, and capacity (for Knapsack), or a list of sets and the universe (for Set Covering).

    ---
    Problem Statement:
    \"\"\"{problem_statement}\"\"\"

    ---

    Respond with a Python dictionary in the following format:

    {{
        "problem_type": "tsp" or "assignment" or "knapsack" or "set_covering" or "unknown",
        "data": {{
            "distance_matrix": [[...], [...], ...],
            "cost_matrix": [[...], [...], ...],
            "values": [...], "weights": [...], "capacity": ..., "item_names": [...],
            "sets": [[...], [...], ...],  # List of sets
            "universe": [...]             # The universe of elements
        }},
        "error": None
    }}

    If you cannot reliably identify the problem type or extract the data, set "problem_type" to "unknown" and provide an error message.  The data should be a valid Python list of lists (for matrices) or a list/number as appropriate.  Do not include any explanation or extra text.
    """
    try:
        response = model.generate_content(prompt)
        count.append(model.count_tokens(response.text).total_tokens)
        match = re.search(r'\{.*\}', response.text, re.DOTALL)
        if match:
            return eval(match.group(0))
        else:
            return {"problem_type": "unknown", "data": {}, "error": "Failed to parse AI response."}
    except Exception as e:
        print(f"Error in extract_combinatorial_data: {e}")  # Add error logging
        return {"problem_type": "unknown", "data": {}, "error": f"Error during AI processing: {e}"}


def modify_combinatorial(session_com, user_input):
    model = genai.GenerativeModel('models/learnlm-1.5-pro-experimental')
    prompt = f"""
    You are assisting in modifying a combinatorial type optimisation problem. Here is the existing problem in dictionary format:

    {session_com}

    Based on this user instruction:
    "{user_input}"

    Return an updated version of the dictionary **with only the necessary changes made**.
        DO NOT remove or omit any fields from the original unless asked explicitly. Maintain structure integrity.

    Return ONLY the Python dictionary with changes implemented. No explanation or extra text.
    """
    response = model.generate_content(prompt)
    count.append(model.count_tokens(response.text).total_tokens)
    try:
        match = re.search(r'\{.*\}', response.text, re.DOTALL)
        if match:
            return eval(match.group(0)), None
        else:
            return None, "Failed to parse modified combinatorial type problem."
    except Exception as e:
        return None, f"Error parsing modified combinatorial type problem: {e}"


def extract_stochastic_components(problem_text):
    prompt = f"""
    You are an expert in stochastic optimization.

    From the following problem statement, extract:

    - **First-stage variables**: Variables decided before uncertainty is realized.
    - **Second-stage variables**: Variables decided after uncertainty is realized.
    - **Scenarios**: Each scenario should have:
      - Name
      - Probability
      - Scenario-specific cost coefficients
      - Scenario-specific constraints

    ---
    Problem:
    \"\"\"{problem_text}\"\"\"
    ---

    Format the output strictly like this Python dictionary:

    {{
      "first_stage_variables": ["x1", "x2", ...],
      "second_stage_variables": ["y1", "y2", ...],
      "scenarios": [
        {{
          "name": "High Demand",
          "probability": 0.6,
          "cost_coefficients": [float, float, ...],
          "constraints": ["constraint 1 description", "constraint 2 description", ...]
        }},
        ...
      ],
      "objective": "maximize" or "minimize"
    }}

    Only return the dictionary. No explanations.
    """
    try:
        response = model.generate_content(prompt)
        match = re.search(r'\{.*\}', response.text, re.DOTALL)
        if match:
            return eval(match.group(0))
        else:
            return {"error": "Failed to parse stochastic components."}
    except Exception as e:
        return {"error": f"Exception during stochastic extraction: {e}"}


def solve_two_stage_stochastic(stochastic_data):
    try:
        first_stage_vars = stochastic_data["first_stage_variables"]
        second_stage_vars = stochastic_data["second_stage_variables"]
        scenarios = stochastic_data["scenarios"]
        objective = stochastic_data["objective"]

        num_x = len(first_stage_vars)
        num_y = len(second_stage_vars)
        num_scenarios = len(scenarios)

        total_vars = num_x + (num_y * num_scenarios)

        # Define full decision vector z = [x, y_s1, y_s2, ..., y_sn]

        def expected_total_value(z):
            x = z[:num_x]  # First-stage
            total = 0.0

            for i, scen in enumerate(scenarios):
                prob = scen["probability"]
                cost_coeffs = scen["cost_coefficients"]
                y_s = z[num_x + i * num_y: num_x + (i + 1) * num_y]  # Slice y for scenario i

                scen_cost = sum(c * xi for c, xi in zip(cost_coeffs, x)) \
                            + sum(c * yi for c, yi in zip(cost_coeffs, y_s))  # Example: assume same cost structure

                total += prob * scen_cost

            return -total if objective == "maximize" else total

        # Default bounds: no negative values
        bounds = [(0, None) for _ in range(total_vars)]

        # Solve
        result = minimize(expected_total_value, [1.0] * total_vars, bounds=bounds, method='SLSQP')

        if result.success:
            optimal_value = -result.fun if objective == "maximize" else result.fun
            optimal_x = result.x[:num_x]
            optimal_ys = [result.x[num_x + i * num_y: num_x + (i + 1) * num_y] for i in range(num_scenarios)]
            return None, optimal_value, optimal_x, optimal_ys
        else:
            return f"Two-stage stochastic optimization failed: {result.message}", None, None, None

    except Exception as e:
        return f"Error solving two-stage stochastic with second-stage: {e}", None, None, None


def classify_user_input(user_input, session_problem=None):
    prompt = f"""
    You are an intelligent assistant that classifies user instructions in the context of optimization problems.

    Given the following current problem description (if any):
    \"\"\"{session_problem or 'None'}\"\"\"

    And the user's latest input:
    \"\"\"{user_input}\"\"\"

    Decide whether the user's input is:
    - A completely **new optimization problem**, or
    - A **modification or follow-up** to the existing one.

    Return only one word as a response:
    "new" ‚Üí if it's a new problem
    "followup" ‚Üí if it's related to the existing one
    """

    try:
        response = model.generate_content(prompt)
        count.append(model.count_tokens(response.text).total_tokens)
        answer = response.text.strip().lower()
        if "new" in answer:
            return None
        elif "followup" in answer:
            return st.session_state.problem_type
        else:
            return "unknown"
    except Exception as e:
        return f"error: {e}"


# Step 1: Detect if compound interest needs to be applied
def contains_compound_interest_terms(text):
    interest_keywords = ["interest", "returns", "compounded", "growth over", "rate"]
    return any(keyword in text.lower() for keyword in interest_keywords)


# Step 2: Use LLM to extract (rate, years) for each asset type
def extract_growth_coefficients_with_llm(text):
    prompt = f"""
    From the following optimization problem, extract compound growth coefficients for each asset class:

    ---
    {text}
    ---

    For each asset type mentioned (e.g., equity mutual funds, real estate, fixed deposits), return a dictionary:
    {{
        "name": <asset name>,
        "rate": <annual return as decimal>,
        "years": <time horizon in years>
    }}

    Respond as a Python list of dictionaries, e.g.:
    [
        {{"name": "equity", "rate": 0.12, "years": 7}},
        {{"name": "real_estate", "rate": 0.10, "years": 7}},
        {{"name": "fixed", "rate": 0.055, "years": 7}}
    ]
    """
    model = genai.GenerativeModel('models/learnlm-1.5-pro-experimental')
    response = model.generate_content(prompt)
    match = re.search(r'\[.*\]', response.text, re.DOTALL)
    if match:
        try:
            return eval(match.group(0))
        except:
            return []
    return []


# Step 3: Replace coefficients in text based on growth calculation (1 + r)^t
def inject_compound_growth_coefficients(text):
    if not contains_compound_interest_terms(text):
        return text

    growth_entries = extract_growth_coefficients_with_llm(text)
    growth_map = {}

    for entry in growth_entries:
        name = entry['name'].lower().replace(" ", "_")
        rate = entry['rate']
        years = entry['years']
        growth_factor = round((1 + rate) ** years, 4)
        growth_map[name] = growth_factor

    # Append to input as objective if needed, or LLM will use it during LPP extraction
    summary = "\n\n# Calculated Growth Coefficients:\n"
    for asset, factor in growth_map.items():
        summary += f"{asset} => {factor}\n"

    return text + summary


def handle_missing_information(problem_type, session_problem, user_input, detect_type_failed=False):
    """
    Handles missing information in the problem statement by prompting the user for input,
    updating the problem statement, and solving the updated problem.

    Args:
        problem_type (str): The type of the problem (e.g., "linear", "nonlinear", etc.).
        session_problem (dict): The current problem dictionary.
        user_input (str): The latest user input.
        detect_type_failed (bool): Whether the optimization type detection failed.

    Returns:
        None
    """
    model = genai.GenerativeModel('models/learnlm-1.5-pro-experimental')

    if detect_type_failed:
        # If optimization type detection failed, ask the user for more information
        st.session_state.history.append(
            ("assistant", "‚ö†Ô∏è Optimization type could not be detected. Please provide more details about the problem."))
        return  # Wait for the user to provide input in the existing text box

    # Handle None case for problem_type
    problem_type_description = problem_type.replace('_', ' ') if problem_type else "unknown optimization"

    # Step 1: Check for missing information
    completeness_check, error_message = check_problem_completeness(problem_type, session_problem, user_input)

    if completeness_check is None:
        st.session_state.history.append(("assistant", f"‚ùå Failed to check problem completeness: {error_message}"))
        return  # Wait for the user to provide input in the existing text box

    if error_message:
        st.session_state.history.append(("assistant", f"‚ö†Ô∏è {error_message}"))

        # Step 2: Prompt the user for missing information
        missing_fields = completeness_check.get("missing_fields", [])
        questions = []
        for field in missing_fields:
            prompt = f"What value should be assigned to '{field}'? Please provide a valid input."
            questions.append(prompt)

        # Display all questions in a single markdown block
        st.session_state.history.append(
            ("assistant", "### Missing Information:\n" + "\n".join([f"- {q}" for q in questions])))
        return  # Wait for the user to provide input in the existing text box

    # If no missing information, proceed to modify the problem
    if problem_type == "linear":
        modified_problem, error = modify_lpp(session_problem, user_input)
        if error:
            st.session_state.history.append(("assistant", f"‚ùå {error}"))
            return
        err2, opt_val, opt_vars = solve_lpp(modified_problem)
    elif problem_type == "nonlinear":
        modified_problem, error = modify_nlp(session_problem, user_input)
        if error:
            st.session_state.history.append(("assistant", f"‚ùå {error}"))
            return
        err2, opt_val, opt_vars = solve_nlp(modified_problem)
    elif problem_type == "stochastic_programming":
        modified_problem, error = modify_combinatorial(session_problem, user_input)
        if error:
            st.session_state.history.append(("assistant", f"‚ùå {error}"))
            return
        err2, opt_val, opt_vars = solve_two_stage_stochastic(modified_problem)
    else:
        st.session_state.history.append(("assistant", "‚ùå Unsupported problem type."))
        return

    # Step 4: Solve the updated problem and display the results
    if err2:
        st.session_state.history.append(("assistant", f"‚ùå {err2}"))
    else:
        if problem_type == "linear":
            formatted = format_solution(opt_val, opt_vars, modified_problem.get("objective"), modified_problem)
        elif problem_type == "nonlinear":
            formatted = format_nlp_solution(opt_val, opt_vars, modified_problem.get("objective_type", "minimize"),
                                            modified_problem)
        elif problem_type == "stochastic_programming":
            formatted = humanize_response(f"Optimal Value: {opt_val}\nVariable Values: {opt_vars}",
                                          "stochastic programming")
        else:
            formatted = "Solution formatted successfully."

        human_response = humanize_response(formatted, problem_type.replace("_", " "))
        st.session_state.session_problem = modified_problem
        st.session_state.history.append(("assistant", human_response))


def check_problem_completeness(problem_type, session_problem, user_input):
    """
    Checks if the current problem statement has enough information to solve the problem.
    If not, prompts the model to identify missing information.

    Args:
        problem_type (str): The type of the problem (e.g., "linear", "nonlinear", etc.).
        session_problem (dict): The current problem dictionary.
        user_input (str): The latest user input.

    Returns:
        dict: Updated problem dictionary with missing fields flagged, if any.
        str: Error message or None if no issues are found.
    """
    # Handle None case for problem_type
    problem_type_description = problem_type.replace('_', ' ') if problem_type else "unknown optimization"

    model = genai.GenerativeModel('models/learnlm-1.5-pro-experimental')
    prompt = f"""
    You are an expert in {problem_type_description} optimization.

    Here is the current problem statement in dictionary format:
    {session_problem}

    Based on the user's latest input:
    "{user_input}"

    Determine if the information provided is sufficient to solve the problem. If any critical information is missing, 
    list the missing fields or details required to proceed.

    Return a Python dictionary in the following format:
    {{
        "is_complete": True or False,
        "missing_fields": ["field1", "field2", ...]  # List of missing fields or details
    }}

    Respond ONLY with the dictionary. Do not include any explanation or extra text.
    """
    try:
        response = model.generate_content(prompt)
        match = re.search(r'\{.*\}', response.text, re.DOTALL)
        if match:
            completeness_check = eval(match.group(0))
            if not completeness_check.get("is_complete", False):
                missing_fields = completeness_check.get("missing_fields", [])
                error_message = f"Missing information: {', '.join(missing_fields)}"
                return completeness_check, error_message
            return completeness_check, None
        else:
            return None, "Failed to parse completeness check response."
    except Exception as e:
        return None, f"Error during completeness check: {e}"




st.markdown("Describe your problem in natural language and let AI extract, solve, and refine it interactively.")

if 'history' not in st.session_state:
    st.session_state.history = []
if 'session_problem' not in st.session_state:
    st.session_state.session_problem = None
if 'problem_type' not in st.session_state:
    st.session_state.problem_type = None  # Initialize problem_type

# --- üì• Excel File Upload and Solve Section (Extended to Smart Detection) ---
st.subheader("üìÑ Optional: Upload an Excel File for Problem Detection")
uploaded_file = st.file_uploader("Upload an Excel file (.xlsx)", type=["xlsx"])

if st.session_state.get("excel_handled"):
    if st.button("üîÑ Clear Uploaded Excel and Start New Problem"):
        del st.session_state.excel_handled
        del st.session_state.session_problem
        del st.session_state.problem_type
        # st.experimental_rerun()

if uploaded_file and 'excel_handled' not in st.session_state:
    try:
        df_dict = pd.read_excel(uploaded_file, sheet_name=None)  # Read all sheets
        st.success("‚úÖ Excel file uploaded successfully.")


        # --- Convert Excel to text for AI detection ---
        def describe_excel_for_ai(df_dict):
            content = ""
            for sheet_name, df in df_dict.items():
                content += f"Sheet: {sheet_name}\n"
                content += df.to_string(index=False)
                content += "\n\n"
            return content


        excel_text = describe_excel_for_ai(df_dict)

        # --- Detect optimization type ---
        opt_types = detect_optimization_type(excel_text)

        if opt_types.get("linear_programming", False):
            user_input = inject_compound_growth_coefficients(user_input)
            lpp_data, error = extract_lpp_from_text(user_input)
            if error:
                st.error(f"‚ùå {error}")
            else:
                err2, opt_val, opt_vars = solve_lpp(lpp_data)
                if err2:
                    st.error(f"‚ùå {err2}")
                else:
                    formatted = format_solution(opt_val, opt_vars, lpp_data.get("objective"), lpp_data)
                    human_response = humanize_response(formatted)

                    st.session_state.session_problem = lpp_data
                    st.session_state.problem_type = "linear"
                    st.session_state.history.append(
                        ("assistant", "üìÑ **Excel file loaded and processed as Linear Programming!**"))
                    st.session_state.history.append(("assistant", human_response))

        elif opt_types.get("nonlinear_programming", False):
            nlp_data, error = extract_nlp_components(excel_text)
            if error:
                st.error(f"‚ùå {error}")
            else:
                err2, opt_val, opt_vars = solve_nlp(nlp_data)
                if err2:
                    st.error(f"‚ùå {err2}")
                else:
                    nlp_data['objective_function_original'] = \
                    re.search(r"lambda x: (.+)", nlp_data.get('objective_function', ''))[1] if nlp_data.get(
                        'objective_function') else "Objective Function"
                    formatted = format_nlp_solution(opt_val, opt_vars, nlp_data.get("objective_type", "minimize"),
                                                    nlp_data)
                    human_response = humanize_response(formatted, "non-linear programming")

                    st.session_state.session_problem = nlp_data
                    st.session_state.problem_type = "nonlinear"
                    st.session_state.history.append(
                        ("assistant", "üìÑ **Excel file loaded and processed as Nonlinear Programming!**"))
                    st.session_state.history.append(("assistant", human_response))


        elif opt_types.get("stochastic_programming", False):
            stochastic_dict = extract_stochastic_components(excel_text)
            if "error" in stochastic_dict:
                st.error(f"‚ùå {stochastic_dict['error']}")
            else:
                err2, optimal_value, optimal_x, optimal_ys = solve_two_stage_stochastic(stochastic_dict)
                if err2:
                    st.error(f"‚ùå {err2}")
                else:
                    # Format the solution
                    var_names = stochastic_dict["first_stage_variables"]
                    second_var_names = stochastic_dict["second_stage_variables"]
                    num_scenarios = len(stochastic_dict["scenarios"])

                    var_details = "\n".join([f"  - {name}: {val:.2f}" for name, val in zip(var_names, optimal_x)])

                    second_stage_details = ""
                    for i in range(num_scenarios):
                        scen_name = stochastic_dict["scenarios"][i]["name"]
                        second_decisions = optimal_ys[i]
                        decisions_text = ", ".join(
                            [f"{var}: {val:.2f}" for var, val in zip(second_var_names, second_decisions)])
                        second_stage_details += f"\n**Scenario {i + 1} ({scen_name}):** {decisions_text}"

                    summary = f"**Optimal Expected Value:** {optimal_value:.2f}\n\n**First-Stage Decisions:**\n{var_details}\n\n**Second-Stage Decisions:**{second_stage_details}"
                    human_response = humanize_response(summary, "two-stage stochastic programming")

                    st.session_state.session_problem = stochastic_dict
                    st.session_state.problem_type = "stochastic_programming"
                    st.session_state.history.append(
                        ("assistant", "üìÑ **Excel file loaded and processed as Stochastic Programming!**"))
                    st.session_state.history.append(("assistant", human_response))



        else:
            # --- Try detecting combinatorial optimization ---
            combinatorial_data = extract_combinatorial_data(excel_text)

            if combinatorial_data["problem_type"] == "tsp":
                st.session_state.problem_type = "combinatorial_tsp"
                distance_matrix = np.array(combinatorial_data["data"].get("distance_matrix", []))
                result = solve_tsp(distance_matrix)
                if isinstance(result, str):
                    st.error(f"‚ùå {result}")
                else:
                    distance, path = result
                    formatted = format_tsp_solution(distance, path)
                    human_response = humanize_response(formatted, "traveling salesman problem")
                    st.session_state.session_problem = combinatorial_data
                    st.session_state.history.append(("assistant", "üìÑ **Excel file loaded and processed as TSP!**"))
                    st.session_state.history.append(("assistant", human_response))

            elif combinatorial_data["problem_type"] == "assignment":
                st.session_state.problem_type = "combinatorial_assignment"
                cost_matrix = np.array(combinatorial_data["data"].get("cost_matrix", []))
                result = solve_assignment_problem(cost_matrix)
                if isinstance(result, str):
                    st.error(f"‚ùå {result}")
                else:
                    cost, assignment = result
                    formatted = format_assignment_solution(cost, assignment)
                    human_response = humanize_response(formatted, "assignment problem")
                    st.session_state.session_problem = combinatorial_data
                    st.session_state.history.append(
                        ("assistant", "üìÑ **Excel file loaded and processed as Assignment Problem!**"))
                    st.session_state.history.append(("assistant", human_response))

            elif combinatorial_data["problem_type"] == "knapsack":
                st.session_state.problem_type = "combinatorial_knapsack"
                values = combinatorial_data["data"].get("values", [])
                weights = combinatorial_data["data"].get("weights", [])
                capacity = combinatorial_data["data"].get("capacity")
                item_names = combinatorial_data["data"].get("item_names", [])
                optimal_value, chosen_items = solve_knapsack(values, weights, capacity, item_names)
                formatted_solution = f"Optimal Knapsack Value: {optimal_value}\nChosen Items: {chosen_items}"
                human_response = humanize_response(formatted_solution, "knapsack problem")
                st.session_state.session_problem = combinatorial_data
                st.session_state.history.append(
                    ("assistant", "üìÑ **Excel file loaded and processed as Knapsack Problem!**"))
                st.session_state.history.append(("assistant", human_response))
                item_names = combinatorial_data["data"].get("item_names", [f"Item {i + 1}" for i in range(len(values))])
                st.success(f"üéØ Optimal Value: {optimal_value}")
                display_knapsack_solution(values, weights, item_names, chosen_items)


            elif combinatorial_data["problem_type"] == "set_covering":
                st.session_state.problem_type = "set_covering"
                sets = combinatorial_data["data"].get("sets", [])
                universe = combinatorial_data["data"].get("universe", [])
                result = solve_set_covering(sets, universe)
                if isinstance(result, str):
                    st.error(f"‚ùå {result}")
                else:
                    cover_indices, cost = result
                    formatted = format_set_covering_solution(cover_indices, cost, sets)
                    human_response = humanize_response(formatted, "set covering problem")
                    st.session_state.session_problem = combinatorial_data
                    st.session_state.history.append(
                        ("assistant", "üìÑ **Excel file loaded and processed as Set Covering Problem!**"))
                    st.session_state.history.append(("assistant", human_response))

            else:
                st.warning("‚ö†Ô∏è Uploaded Excel could not be classified as a supported problem.")

        st.session_state.excel_handled = True

    except Exception as e:
        st.error(f"‚ùå Error processing Excel file: {e}")

user_input = st.chat_input("Enter a new LPP description or a follow-up modification...")

if user_input:
    st.session_state.problem_type = classify_user_input(user_input, st.session_state.session_problem)
    with st.spinner("Processing..."):
        if st.session_state.problem_type is None:
            opt_types = detect_optimization_type(user_input)
            if not any(opt_types.values()):
                handle_missing_information(
                    problem_type=None,
                    session_problem=st.session_state.session_problem,
                    user_input=user_input,
                    detect_type_failed=True
                )
            else:
                # Continue with the regular flow
                st.session_state.problem_type = next(key for key, val in opt_types.items() if val)
                completeness_check, error_message = check_problem_completeness(
                    problem_type=st.session_state.problem_type,
                    session_problem=st.session_state.session_problem,
                    user_input=user_input
                )

                if error_message:
                    handle_missing_information(
                        problem_type=st.session_state.problem_type,
                        session_problem=st.session_state.session_problem,
                        user_input=user_input
                    )
                else:
                    st.success("‚úÖ The problem statement is complete and ready to solve!")
                    if opt_types.get("linear_programming", False):
                        st.session_state.problem_type = "linear"
                        user_input = inject_compound_growth_coefficients(user_input)
                        lpp_data, error = extract_lpp_from_text(user_input)
                        #    if st.session_state.session_problem is None:

                        # or not all(elem in lpp_data.get ("variable_names") for elem in st.session_state.session_problem.get ("variable_names")):
                        st.markdown('New lpp detected')
                        #        modified_lpp, error = extract_lpp_from_text(user_input)
                        if error:
                            st.session_state.history.append(("user", user_input))
                            st.session_state.history.append(("assistant", f"‚ùå {error}"))
                        else:
                            #                while lpp_data.get ("missing"):
                            #                    lpp_data = ask_user_to_fill_missing_values(lpp_data)

                            err2, opt_val, opt_vars = solve_lpp(lpp_data)
                            if err2:
                                st.session_state.history.append(("user", user_input))
                                st.session_state.history.append(("assistant", f"‚ùå {err2}"))
                            else:
                                formatted = format_solution(opt_val, opt_vars, lpp_data.get("objective"), lpp_data)
                                human_response = humanize_response(formatted)
                                st.session_state.session_problem = lpp_data
                                st.session_state.history.append(("user", user_input))
                                st.session_state.history.append(("assistant", human_response))
                                true_types = [key.replace("_", " ").title() for key, val in opt_types.items() if val]
                                if true_types:
                                    st.session_state.history.append(
                                        ("assistant", f"üìö **Detected Optimization Types:** `{', '.join(true_types)}`"))
                                else:
                                    st.session_state.history.append(
                                        ("assistant", "üìö **Detected Optimization Types:** `None detected`"))


                    elif opt_types.get("nonlinear_programming", False):
                        st.session_state.problem_type = "nonlinear"
                        nlp_data, error = extract_nlp_components(user_input)
                        if error:
                            st.session_state.history.append(("user", user_input))
                            st.session_state.history.append(("assistant", f"‚ùå {error}"))
                        else:
                            err2, opt_val, opt_vars = solve_nlp(nlp_data)
                            if err2:
                                st.session_state.history.append(("user", user_input))
                                st.session_state.history.append(("assistant", f"‚ùå {err2}"))
                            else:
                                # Store the original objective function string for better output
                                nlp_data['objective_function_original'] = \
                                re.search(r"lambda x: (.+)", nlp_data.get('objective_function', ''))[1] if nlp_data.get(
                                    'objective_function') else "Objective Function"
                                formatted = format_nlp_solution(opt_val, opt_vars,
                                                                nlp_data.get("objective_type", "minimize"), nlp_data)
                                human_response = humanize_response(formatted, "non-linear programming")
                                st.session_state.session_problem = nlp_data
                                st.session_state.history.append(("user", user_input))
                                st.session_state.history.append(("assistant", human_response))
                                true_types = [key.replace("_", " ").title() for key, val in opt_types.items() if val]
                                if true_types:
                                    st.session_state.history.append(
                                        ("assistant", f"üìö **Detected Optimization Types:** {', '.join(true_types)}"))
                                else:
                                    st.session_state.history.append(
                                        ("assistant", "üìö **Detected Optimization Types:** None detected"))


                    elif opt_types.get("combinatorial_optimization", False):
                        # Use AI to extract the problem type and data
                        combinatorial_data = extract_combinatorial_data(user_input)
                        st.session_state.session_problem = combinatorial_data

                        if combinatorial_data["problem_type"] == "tsp":
                            st.session_state.problem_type = "combinatorial_tsp"
                            distance_matrix = np.array(combinatorial_data["data"].get("distance_matrix", []))
                            result = solve_tsp(distance_matrix)

                            if isinstance(result, str):
                                error = result
                                st.session_state.history.append(("user", user_input))
                                st.session_state.history.append(("assistant", f"‚ùå {error}"))
                            else:
                                distance, path = result
                                formatted_solution = format_tsp_solution(distance, path)
                                human_response = humanize_response(formatted_solution, "traveling salesman problem")
                                st.session_state.history.append(("user", user_input))
                                st.session_state.history.append(("assistant", human_response))

                        elif combinatorial_data["problem_type"] == "assignment":
                            st.session_state.problem_type = "combinatorial_assignment"
                            cost_matrix = np.array(combinatorial_data["data"].get("cost_matrix", []))
                            result = solve_assignment_problem(cost_matrix)

                            if isinstance(result, str):
                                error = result
                                st.session_state.history.append(("user", user_input))
                                st.session_state.history.append(("assistant", f"‚ùå {error}"))
                            else:
                                cost, assignment = result
                                formatted_solution = format_assignment_solution(cost, assignment)
                                human_response = humanize_response(formatted_solution, "assignment problem")
                                st.session_state.history.append(("user", user_input))
                                st.session_state.history.append(("assistant", human_response))

                        elif combinatorial_data["problem_type"] == "knapsack":
                            st.session_state.problem_type = "combinatorial_knapsack"
                            values = combinatorial_data["data"].get("values", [])
                            weights = combinatorial_data["data"].get("weights", [])
                            capacity = combinatorial_data["data"].get("capacity")
                            item_names = combinatorial_data["data"].get("item_names", [])

                            try:
                                optimal_value, chosen_items = solve_knapsack(values, weights, capacity, item_names)
                                formatted_solution = f"Optimal Knapsack Value: {optimal_value}\nChosen Items: {chosen_items}"
                                human_response = humanize_response(formatted_solution, "knapsack problem")
                                st.session_state.history.append(("user", user_input))
                                st.session_state.history.append(("assistant", human_response))
                                item_names = combinatorial_data["data"].get("item_names", [f"Item {i + 1}" for i in
                                                                                           range(len(values))])
                                st.success(f"üéØ Optimal Value: {optimal_value}")
                                display_knapsack_solution(values, weights, item_names, chosen_items)


                            except Exception as e:
                                st.session_state.history.append(("user", user_input))
                                st.session_state.history.append(("assistant", f"‚ùå Error solving knapsack problem: {e}"))

                        elif combinatorial_data["problem_type"] == "set_covering":
                            st.session_state.problem_type = "set_covering"
                            sets = combinatorial_data["data"].get("sets", [])
                            universe = combinatorial_data["data"].get("universe", [])

                            if not sets or not universe:
                                error_message = "Error: Set Covering Problem requires both 'sets' and 'universe' data."
                                st.session_state.history.append(("user", user_input))
                                st.session_state.history.append(("assistant", f"‚ùå {error_message}"))
                            else:
                                result = solve_set_covering(sets, universe)
                                if isinstance(result, str):
                                    error_message = result
                                    st.session_state.history.append(("user", user_input))
                                    st.session_state.history.append(("assistant", f"‚ùå {error_message}"))
                                else:
                                    cover_indices, cost = result
                                    formatted_solution = format_set_covering_solution(cover_indices, cost, sets)
                                    human_response = humanize_response(formatted_solution, "set covering problem")
                                    st.session_state.history.append(("user", user_input))
                                    st.session_state.history.append(("assistant", human_response))




                        else:
                            st.session_state.history.append(("user", user_input))
                            st.session_state.history.append(("assistant",
                                                             f"‚ùå Could not identify combinatorial problem type.  Error: {combinatorial_data['error']}"))

                    elif opt_types.get("stochastic_programming", False):
                        st.session_state.problem_type = "stochastic_programming"
                        stochastic_data = extract_stochastic_components(user_input)

                        if "error" in stochastic_data:
                            st.session_state.history.append(("user", user_input))
                            st.session_state.history.append(("assistant", f"‚ùå {stochastic_data['error']}"))
                        else:
                            err2, optimal_value, optimal_x, optimal_ys = solve_two_stage_stochastic(stochastic_data)

                            if err2:
                                st.session_state.history.append(("user", user_input))
                                st.session_state.history.append(("assistant", f"‚ùå {err2}"))
                            else:
                                var_names = stochastic_data["first_stage_variables"]
                                second_var_names = stochastic_data["second_stage_variables"]
                                num_scenarios = len(stochastic_data["scenarios"])

                                var_details = "\n".join(
                                    [f"  - {name}: {val:.2f}" for name, val in zip(var_names, optimal_x)])

                                second_stage_details = ""
                                for i in range(num_scenarios):
                                    scen_name = stochastic_data["scenarios"][i]["name"]
                                    second_decisions = optimal_ys[i]
                                    decisions_text = ", ".join(
                                        [f"{var}: {val:.2f}" for var, val in zip(second_var_names, second_decisions)])
                                    second_stage_details += f"\n**Scenario {i + 1} ({scen_name}):** {decisions_text}"

                                summary = f"**Optimal Expected Value:** {optimal_value:.2f}\n\n**First-Stage Decisions:**\n{var_details}\n\n**Second-Stage Decisions:**{second_stage_details}"

                                human_response = humanize_response(summary, "two-stage stochastic programming")
                                st.session_state.session_problem = stochastic_data
                                st.session_state.history.append(("user", user_input))
                                st.session_state.history.append(("assistant", human_response))





        ##################################
        else:
            if st.session_state.problem_type == "linear":
                user_input = inject_compound_growth_coefficients(user_input)
                modified_lpp, error = modify_lpp(st.session_state.session_problem, user_input)
                if error:
                    st.session_state.history.append(("user", user_input))
                    st.session_state.history.append(("assistant", f"‚ùå {error}"))
                else:
                    err2, opt_val, opt_vars = solve_lpp(modified_lpp)
                    if err2:
                        st.session_state.history.append(("user", user_input))
                        st.session_state.history.append(("assistant", f"‚ùå {err2}"))
                    else:
                        formatted = format_solution(opt_val, opt_vars, modified_lpp.get("objective"), modified_lpp)
                        human_response = humanize_response(formatted, "linear programming")
                        st.session_state.session_problem = modified_lpp
                        st.session_state.history.append(("user", user_input))
                        st.session_state.history.append(("assistant", human_response))




            elif st.session_state.problem_type == "nonlinear":
                modified_nlp, error = modify_nlp(st.session_state.session_problem, user_input)
                if error:
                    st.session_state.history.append(("user", user_input))
                    st.session_state.history.append(("assistant", f"‚ùå {error}"))
                else:
                    err2, opt_val, opt_vars = solve_nlp(modified_nlp)
                    if err2:
                        st.session_state.history.append(("user", user_input))
                        st.session_state.history.append(("assistant", f"‚ùå {err2}"))
                    else:
                        formatted = format_nlp_solution(opt_val, opt_vars,
                                                        modified_nlp.get("objective_type", "minimize"), modified_nlp)
                        human_response = humanize_response(formatted, "non-linear programming")
                        st.session_state.session_problem = modified_nlp
                        st.session_state.history.append(("user", user_input))
                        st.session_state.history.append(("assistant", human_response))




            elif st.session_state.problem_type == "combinatorial_knapsack":
                modified_knapsack, error = modify_combinatorial(st.session_state.session_problem, user_input)

                if error or modified_knapsack is None:
                    st.session_state.history.append(("user", user_input))
                    st.session_state.history.append(("assistant", f"‚ùå {error}"))
                else:
                    values = modified_knapsack["data"].get("values", [])
                    weights = modified_knapsack["data"].get("weights", [])
                    capacity = modified_knapsack["data"].get("capacity")
                    item_names = modified_knapsack["data"].get("item_names", [])

                    optimal_value, chosen_items = solve_knapsack(values, weights, capacity, item_names)
                    formatted_solution = f"Optimal Knapsack Value: {optimal_value}\nChosen Items: {chosen_items}"
                    human_response = humanize_response(formatted_solution, "knapsack problem")
                    st.session_state.problem_type = modified_knapsack
                    st.session_state.history.append(("user", user_input))
                    st.session_state.history.append(("assistant", human_response))
                    item_names = modified_knapsack["data"].get("item_names",
                                                               [f"Item {i + 1}" for i in range(len(values))])
                    st.success(f"üéØ Optimal Value: {optimal_value}")
                    display_knapsack_solution(values, weights, item_names, chosen_items)






            elif st.session_state.problem_type == "combinatorial_assignment":
                cost_matrix = np.array(st.session_state.session_problem["data"].get("cost_matrix", []))
                modified_assignment, error = modify_combinatorial(st.session_state.session_problem, user_input)

                if error or modified_assignment is None:
                    st.session_state.history.append(("user", user_input))
                    st.session_state.history.append(("assistant", f"‚ùå {error}"))
                else:
                    cost_matrix = np.array(modified_assignment["data"].get("cost_matrix", []))
                    optimal_cost, assignment = solve_assignment_problem(cost_matrix)

                    if isinstance(optimal_cost, str):
                        st.session_state.history.append(("user", user_input))
                        st.session_state.history.append(("assistant", f"‚ùå {optimal_cost}"))
                    else:
                        formatted = format_assignment_solution(optimal_cost, assignment)
                        human_response = humanize_response(formatted, "assignment problem")
                        st.session_state.session_problem = modified_assignment
                        st.session_state.history.append(("user", user_input))
                        st.session_state.history.append(("assistant", human_response))


            elif st.session_state.problem_type == "combinatorial_tsp":
                distance_matrix = np.array(st.session_state.session_problem["data"].get("distance_matrix", []))
                modified_tsp, error = modify_combinatorial(st.session_state.session_problem, user_input)


if 'history' not in st.session_state:
    st.session_state.history = []
if 'session_problem' not in st.session_state:
    st.session_state.session_problem = None
if 'problem_type' not in st.session_state:
    st.session_state.problem_type = None

# ... (rest of your existing optimization code) ...

# Note: I've kept all your existing optimization functions and logic exactly the same
# The only changes are:
# 1. Added proper LangChain integration
# 2. Added mode switching
# 3. Improved the general chat functionality
# 4. Added fallback to Gemini when LangChain can't answer
# 5. Organized the UI better

# The optimization functionality remains identical to your original code